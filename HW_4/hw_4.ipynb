{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/Collection5/'\n",
    "records = load_ne5(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'id', 1: 'text', 2: 'ner'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Жириновский предлагает обменять с США Сноудена...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Д.Медведев назначил ряд глав региональных МВД\\...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>СМИ: В.Суркову надоело работать в администраци...</td>\n",
       "      <td>[Ne5Span(index='T1', type='MEDIA', start=0, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Д.Медведев освободил от должности еще 10 генер...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Жириновский предлагает обменять с США Сноудена...   \n",
       "1  Д.Медведев назначил ряд глав региональных МВД\\...   \n",
       "2  СМИ: В.Суркову надоело работать в администраци...   \n",
       "3  Д.Медведев освободил от должности еще 10 генер...   \n",
       "\n",
       "                                                 ner  \n",
       "0  [Ne5Span(index='T1', type='PER', start=0, stop...  \n",
       "1  [Ne5Span(index='T1', type='PER', start=0, stop...  \n",
       "2  [Ne5Span(index='T1', type='MEDIA', start=0, st...  \n",
       "3  [Ne5Span(index='T1', type='PER', start=0, stop...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 5 типов сущностей: ['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'PER']\n",
      "Всего 11 меток: {0: 'O', 1: 'B-GEOPOLIT', 2: 'I-GEOPOLIT', 3: 'B-LOC', 4: 'I-LOC', 5: 'B-MEDIA', 6: 'I-MEDIA', 7: 'B-ORG', 8: 'I-ORG', 9: 'B-PER', 10: 'I-PER'}\n"
     ]
    }
   ],
   "source": [
    "# Получим все уникальные типы именованных сущностей\n",
    "entity_types = set()\n",
    "for spans in df['ner']:\n",
    "    for span in spans:\n",
    "        entity_types.add(span.type)\n",
    "\n",
    "# Создадим маппинг для меток\n",
    "id2label = {0: \"O\"}  # O - для токенов не входящих в именованную сущность\n",
    "for i, entity_type in enumerate(sorted(entity_types), 1):\n",
    "    id2label[2*i-1] = f\"B-{entity_type}\"  # B- префикс для начала сущности\n",
    "    id2label[2*i] = f\"I-{entity_type}\"    # I- префикс для продолжения сущности\n",
    "    \n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"Всего {len(entity_types)} типов сущностей: {sorted(entity_types)}\")\n",
    "print(f\"Всего {len(id2label)} меток: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования текста и NER-разметки в формат для обучения\n",
    "def preprocess_data(examples):\n",
    "    # Получаем текст и аннотации\n",
    "    text = examples[\"text\"]\n",
    "    spans = examples[\"ner\"]\n",
    "    \n",
    "    # Создаем список меток для каждого символа в тексте (по умолчанию 'O')\n",
    "    char_labels = [\"O\"] * len(text)\n",
    "    \n",
    "    # Заполняем метки для сущностей\n",
    "    for span in spans:\n",
    "        entity_type = span.type\n",
    "        start, end = span.start, span.stop\n",
    "        \n",
    "        # Устанавливаем B- для первого символа\n",
    "        char_labels[start] = f\"B-{entity_type}\"\n",
    "        \n",
    "        # Устанавливаем I- для остальных символов в сущности\n",
    "        for i in range(start + 1, end):\n",
    "            char_labels[i] = f\"I-{entity_type}\"\n",
    "    \n",
    "    # Токенизируем текст\n",
    "    tokenized = tokenizer(text, truncation=True, return_offsets_mapping=True)\n",
    "    labels = []\n",
    "    \n",
    "    # Преобразуем символьные метки в метки токенов\n",
    "    for i, (start, end) in enumerate(tokenized.offset_mapping):\n",
    "        # Пропуск специальных токенов ([CLS], [SEP], ...)\n",
    "        if start == end == 0:\n",
    "            labels.append(-100)  # -100 игнорируется при вычислении потерь\n",
    "            continue\n",
    "            \n",
    "        # Находим наиболее частую метку для символов в этом токене\n",
    "        token_labels = char_labels[start:end]\n",
    "        if not token_labels:\n",
    "            labels.append(-100)\n",
    "            continue\n",
    "            \n",
    "        # Если есть хотя бы одна B- метка, то используем её\n",
    "        b_labels = [l for l in token_labels if l.startswith(\"B-\")]\n",
    "        if b_labels:\n",
    "            labels.append(label2id[b_labels[0]])\n",
    "        else:\n",
    "            # Если есть хотя бы одна I- метка, то используем первую I- метку\n",
    "            i_labels = [l for l in token_labels if l.startswith(\"I-\")]\n",
    "            if i_labels:\n",
    "                labels.append(label2id[i_labels[0]])\n",
    "            else:\n",
    "                # Иначе используем \"O\"\n",
    "                labels.append(label2id[\"O\"])\n",
    "    \n",
    "    # Удаляем offset_mapping, так как он не нужен для обучения\n",
    "    tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-GEOPOLIT': 1,\n",
       " 'I-GEOPOLIT': 2,\n",
       " 'B-LOC': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-MEDIA': 5,\n",
       " 'I-MEDIA': 6,\n",
       " 'B-ORG': 7,\n",
       " 'I-ORG': 8,\n",
       " 'B-PER': 9,\n",
       " 'I-PER': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем функцию к обучающей и тестовой выборкам\n",
    "processed_train_data = []\n",
    "processed_test_data = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    processed_train_data.append(preprocess_data(row))\n",
    "    \n",
    "for i, row in test_df.iterrows():\n",
    "    processed_test_data.append(preprocess_data(row))\n",
    "\n",
    "# Создаем датасеты Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_train_data],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_train_data],\n",
    "    \"labels\": [x[\"labels\"] for x in processed_train_data]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_test_data],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_test_data],\n",
    "    \"labels\": [x[\"labels\"] for x in processed_test_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления метрик\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Убираем игнорируемые индексы\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3831734657287598,\n",
       " 'eval_model_preparation_time': 0.0004,\n",
       " 'eval_precision': 0.006539280014186913,\n",
       " 'eval_recall': 0.057071000193461015,\n",
       " 'eval_f1': 0.01173405461307452,\n",
       " 'eval_accuracy': 0.12865108868826342,\n",
       " 'eval_runtime': 4.9607,\n",
       " 'eval_samples_per_second': 40.317,\n",
       " 'eval_steps_per_second': 5.04}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оценка метрик без дообучения\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models',\n",
    "    per_device_eval_batch_size=8,\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Метрики до дообучения:\")\n",
    "pre_training_metrics = trainer.evaluate(test_dataset)\n",
    "pre_training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_28611/2943834762.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588897</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.250339</td>\n",
       "      <td>0.325617</td>\n",
       "      <td>0.843717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423745</td>\n",
       "      <td>0.441952</td>\n",
       "      <td>0.369704</td>\n",
       "      <td>0.402612</td>\n",
       "      <td>0.868262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386830</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.444960</td>\n",
       "      <td>0.458122</td>\n",
       "      <td>0.885422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6780633036295572, metrics={'train_runtime': 488.9889, 'train_samples_per_second': 4.908, 'train_steps_per_second': 0.614, 'total_flos': 22374206344752.0, 'train_loss': 0.6780633036295572, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3868299722671509,\n",
       " 'eval_precision': 0.4720853858784893,\n",
       " 'eval_recall': 0.444960340491391,\n",
       " 'eval_f1': 0.45812170102579425,\n",
       " 'eval_accuracy': 0.8854221986192247,\n",
       " 'eval_runtime': 4.3761,\n",
       " 'eval_samples_per_second': 45.703,\n",
       " 'eval_steps_per_second': 5.713,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оценка метрик после дообучения\n",
    "post_training_metrics = trainer.evaluate(test_dataset)\n",
    "post_training_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   MLM дообучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Применяем функцию к обучающей и тестовой выборкам\n",
    "processed_train_data_mlm = []\n",
    "processed_test_data_mlm = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    processed_train_data_mlm.append(preprocess_function(row))\n",
    "    \n",
    "for i, row in test_df.iterrows():\n",
    "    processed_test_data_mlm.append(preprocess_function(row))\n",
    "\n",
    "# Создаем датасеты Hugging Face\n",
    "train_dataset_mlm = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_train_data_mlm],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_train_data_mlm],\n",
    "})\n",
    "\n",
    "test_dataset_mlm = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_test_data_mlm],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_test_data_mlm],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce5a1822861441ba03968cf9493d434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f529877c572b4606a8d6072fefa9f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_mlm_grouped = train_dataset_mlm.map(group_texts, batched=True, num_proc=4)\n",
    "test_dataset_mlm_grouped = test_dataset_mlm.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 05:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.191179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.124142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.100579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=372, training_loss=3.4454778855846775, metrics={'train_runtime': 317.2932, 'train_samples_per_second': 18.683, 'train_steps_per_second': 1.172, 'total_flos': 11310209703936.0, 'train_loss': 3.4454778855846775, 'epoch': 3.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/models/rubert-tiny2-mlm\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_mlm_grouped,\n",
    "    eval_dataset=test_dataset_mlm_grouped,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 23.54\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER дообучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../data/models/rubert-tiny2-mlm/checkpoint-372 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    '../data/models/rubert-tiny2-mlm/checkpoint-372', \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner-v2',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_28611/2943834762.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.576352</td>\n",
       "      <td>0.459727</td>\n",
       "      <td>0.260592</td>\n",
       "      <td>0.332634</td>\n",
       "      <td>0.843949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413024</td>\n",
       "      <td>0.442426</td>\n",
       "      <td>0.390985</td>\n",
       "      <td>0.415118</td>\n",
       "      <td>0.876958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375955</td>\n",
       "      <td>0.491532</td>\n",
       "      <td>0.471658</td>\n",
       "      <td>0.481390</td>\n",
       "      <td>0.895031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6769447835286458, metrics={'train_runtime': 487.8083, 'train_samples_per_second': 4.92, 'train_steps_per_second': 0.615, 'total_flos': 22374206344752.0, 'train_loss': 0.6769447835286458, 'epoch': 3.0})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сравнительно с простым дообучением на ner задачу, получили метрики выше на каждой эпохе обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# обучение с доп разметкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_10000_labeled = pd.read_csv('../data/lenta_10000_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_data(text, entities):\n",
    "    # Получаем токены из текста\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, padding=False, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    \n",
    "    # Инициализируем метки для каждого токена как \"O\" (outside)\n",
    "    labels = [\"O\"] * len(input_ids)\n",
    "    \n",
    "    # Создаем словарь для маппинга позиции в тексте к индексу токена\n",
    "    position_to_token_idx = {}\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if start != end:  # Пропускаем спецтокены с нулевой длиной\n",
    "            for pos in range(start, end):\n",
    "                position_to_token_idx[pos] = i\n",
    "    \n",
    "    # Размечаем токены на основе данных entities\n",
    "    for entity in entities:\n",
    "        # Получаем позиции в тексте\n",
    "        start_pos = entity['start']\n",
    "        end_pos = entity['end']\n",
    "        entity_label = entity['entity']\n",
    "        \n",
    "        # Находим соответствующие токены\n",
    "        token_indices = []\n",
    "        for pos in range(start_pos, end_pos):\n",
    "            if pos in position_to_token_idx:\n",
    "                token_idx = position_to_token_idx[pos]\n",
    "                if token_idx not in token_indices:\n",
    "                    token_indices.append(token_idx)\n",
    "        \n",
    "        # Если найдены токены, применяем метки\n",
    "        if token_indices:\n",
    "            # Получаем тип сущности из метки\n",
    "            if '-' in entity_label:\n",
    "                _, entity_type = entity_label.split('-', 1)\n",
    "            else:\n",
    "                continue  # Пропускаем, если метка не в формате X-TYPE\n",
    "            \n",
    "            # Применяем метки в зависимости от кол-ва токенов\n",
    "            for i, token_idx in enumerate(token_indices):\n",
    "                if len(token_indices) == 1:\n",
    "                    labels[token_idx] = f\"U-{entity_type}\"\n",
    "                elif i == 0:\n",
    "                    labels[token_idx] = f\"B-{entity_type}\"\n",
    "                elif i == len(token_indices) - 1:\n",
    "                    labels[token_idx] = f\"L-{entity_type}\"\n",
    "                else:\n",
    "                    labels[token_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": [1] * len(input_ids),\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Функция для конвертации строкового представления списка словарей в список словарей\n",
    "def parse_entities(entities_str):\n",
    "    if isinstance(entities_str, str):\n",
    "        return ast.literal_eval(entities_str)\n",
    "    return entities_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 0 строк из 10000\n",
      "Обработано 100 строк из 10000\n",
      "Обработано 200 строк из 10000\n",
      "Обработано 300 строк из 10000\n",
      "Обработано 400 строк из 10000\n",
      "Обработано 500 строк из 10000\n",
      "Обработано 600 строк из 10000\n",
      "Обработано 700 строк из 10000\n",
      "Обработано 800 строк из 10000\n",
      "Обработано 900 строк из 10000\n",
      "Обработано 1000 строк из 10000\n",
      "Обработано 1100 строк из 10000\n",
      "Обработано 1200 строк из 10000\n",
      "Обработано 1300 строк из 10000\n",
      "Обработано 1400 строк из 10000\n",
      "Обработано 1500 строк из 10000\n",
      "Обработано 1600 строк из 10000\n",
      "Обработано 1700 строк из 10000\n",
      "Обработано 1800 строк из 10000\n",
      "Обработано 1900 строк из 10000\n",
      "Обработано 2000 строк из 10000\n",
      "Обработано 2100 строк из 10000\n",
      "Обработано 2200 строк из 10000\n",
      "Обработано 2300 строк из 10000\n",
      "Обработано 2400 строк из 10000\n",
      "Обработано 2500 строк из 10000\n",
      "Обработано 2600 строк из 10000\n",
      "Обработано 2700 строк из 10000\n",
      "Обработано 2800 строк из 10000\n",
      "Обработано 2900 строк из 10000\n",
      "Обработано 3000 строк из 10000\n",
      "Обработано 3100 строк из 10000\n",
      "Обработано 3200 строк из 10000\n",
      "Обработано 3300 строк из 10000\n",
      "Обработано 3400 строк из 10000\n",
      "Обработано 3500 строк из 10000\n",
      "Обработано 3600 строк из 10000\n",
      "Обработано 3700 строк из 10000\n",
      "Обработано 3800 строк из 10000\n",
      "Обработано 3900 строк из 10000\n",
      "Обработано 4000 строк из 10000\n",
      "Обработано 4100 строк из 10000\n",
      "Обработано 4200 строк из 10000\n",
      "Обработано 4300 строк из 10000\n",
      "Обработано 4400 строк из 10000\n",
      "Обработано 4500 строк из 10000\n",
      "Обработано 4600 строк из 10000\n",
      "Обработано 4700 строк из 10000\n",
      "Обработано 4800 строк из 10000\n",
      "Обработано 4900 строк из 10000\n",
      "Обработано 5000 строк из 10000\n",
      "Обработано 5100 строк из 10000\n",
      "Обработано 5200 строк из 10000\n",
      "Обработано 5300 строк из 10000\n",
      "Обработано 5400 строк из 10000\n",
      "Обработано 5500 строк из 10000\n",
      "Обработано 5600 строк из 10000\n",
      "Обработано 5700 строк из 10000\n",
      "Обработано 5800 строк из 10000\n",
      "Обработано 5900 строк из 10000\n",
      "Обработано 6000 строк из 10000\n",
      "Обработано 6100 строк из 10000\n",
      "Обработано 6200 строк из 10000\n",
      "Обработано 6300 строк из 10000\n",
      "Обработано 6400 строк из 10000\n",
      "Обработано 6500 строк из 10000\n",
      "Обработано 6600 строк из 10000\n",
      "Обработано 6700 строк из 10000\n",
      "Обработано 6800 строк из 10000\n",
      "Обработано 6900 строк из 10000\n",
      "Обработано 7000 строк из 10000\n",
      "Обработано 7100 строк из 10000\n",
      "Обработано 7200 строк из 10000\n",
      "Обработано 7300 строк из 10000\n",
      "Обработано 7400 строк из 10000\n",
      "Обработано 7500 строк из 10000\n",
      "Обработано 7600 строк из 10000\n",
      "Обработано 7700 строк из 10000\n",
      "Обработано 7800 строк из 10000\n",
      "Обработано 7900 строк из 10000\n",
      "Обработано 8000 строк из 10000\n",
      "Обработано 8100 строк из 10000\n",
      "Обработано 8200 строк из 10000\n",
      "Обработано 8300 строк из 10000\n",
      "Обработано 8400 строк из 10000\n",
      "Обработано 8500 строк из 10000\n",
      "Обработано 8600 строк из 10000\n",
      "Обработано 8700 строк из 10000\n",
      "Обработано 8800 строк из 10000\n",
      "Обработано 8900 строк из 10000\n",
      "Обработано 9000 строк из 10000\n",
      "Обработано 9100 строк из 10000\n",
      "Обработано 9200 строк из 10000\n",
      "Обработано 9300 строк из 10000\n",
      "Обработано 9400 строк из 10000\n",
      "Обработано 9500 строк из 10000\n",
      "Обработано 9600 строк из 10000\n",
      "Обработано 9700 строк из 10000\n",
      "Обработано 9800 строк из 10000\n",
      "Обработано 9900 строк из 10000\n"
     ]
    }
   ],
   "source": [
    "# Обработка каждой строки датасета\n",
    "for idx, row in lenta_10000_labeled.iterrows():\n",
    "    try:\n",
    "        text_data = prepare_ner_data(row['text'], parse_entities(row['text_entities']))        \n",
    "        lenta_train_data.append(text_data)\n",
    "            \n",
    "        # Отображение прогресса\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Обработано {idx} строк из {len(lenta_10000_labeled)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке строки {idx}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "lenta_train_dataset = Dataset.from_dict({\n",
    "    'input_ids': [x['input_ids'] for x in lenta_train_data],\n",
    "    'attention_mask': [x['attention_mask'] for x in lenta_train_data],\n",
    "    'labels': [x['labels'] for x in lenta_train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lenta_labels_to_train_format(example):\n",
    "    # Маппинг типов сущностей из lenta в типы train\n",
    "    entity_type_mapping = {\n",
    "        # Географические объекты -> LOC\n",
    "        'CITY': 'LOC',\n",
    "        'COUNTRY': 'LOC',\n",
    "        'DISTRICT': 'LOC',\n",
    "        'REGION': 'LOC',\n",
    "        'STREET': 'LOC',\n",
    "        'HOUSE': 'LOC',  # Дома также относим к локациям\n",
    "        \n",
    "        # Персоны -> PER\n",
    "        'FIRST_NAME': 'PER',\n",
    "        'LAST_NAME': 'PER',\n",
    "        'MIDDLE_NAME': 'PER',\n",
    "        \n",
    "        # Другие типы можно добавить по мере необходимости\n",
    "    }\n",
    "    \n",
    "    # Словарь меток train_dataset\n",
    "    train_label2id = {\n",
    "        'O': 0, \n",
    "        'B-GEOPOLIT': 1, 'I-GEOPOLIT': 2, \n",
    "        'B-LOC': 3, 'I-LOC': 4, \n",
    "        'B-MEDIA': 5, 'I-MEDIA': 6, \n",
    "        'B-ORG': 7, 'I-ORG': 8, \n",
    "        'B-PER': 9, 'I-PER': 10\n",
    "    }\n",
    "    \n",
    "    # Преобразуем метки\n",
    "    new_labels = []\n",
    "    for label in example['labels']:\n",
    "        if label == 'O':\n",
    "            new_labels.append(train_label2id['O'])\n",
    "            continue\n",
    "            \n",
    "        # Разбиваем метку на позицию (B/I/L/U) и тип сущности\n",
    "        position, entity_type = label.split('-', 1)\n",
    "        \n",
    "        # Преобразуем тип сущности\n",
    "        mapped_type = entity_type_mapping.get(entity_type)\n",
    "        if not mapped_type:\n",
    "            # Если нет соответствия, считаем токен не-сущностью\n",
    "            new_labels.append(train_label2id['O'])\n",
    "            continue\n",
    "            \n",
    "        # Преобразуем BIOLU -> BI\n",
    "        if position in ['B', 'U']:  # Начало сущности или единичная сущность -> B\n",
    "            new_label = f'B-{mapped_type}'\n",
    "        elif position in ['I', 'L']:  # Внутри сущности или конец сущности -> I\n",
    "            new_label = f'I-{mapped_type}'\n",
    "        else:\n",
    "            new_label = 'O'\n",
    "            \n",
    "        # Преобразуем в числовой формат\n",
    "        new_labels.append(train_label2id.get(new_label, train_label2id['O']))\n",
    "        \n",
    "    # Обновляем пример\n",
    "    example['labels'] = new_labels\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- оставляем только LOC и PER, тк других похожих на наш целевой датасет сущностей нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414176dfaebd47b59d13e5b6d3222545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenta_train_dataset = lenta_train_dataset.map(convert_lenta_labels_to_train_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставляем только 3000 примеров из датасета\n",
    "lenta_train_dataset = lenta_train_dataset.select(range(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Объединение тренировочных датасетов\n",
    "combined_train_dataset = concatenate_datasets([lenta_train_dataset, train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_38013/3256264858.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1425' max='1425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1425/1425 24:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457694</td>\n",
       "      <td>0.072017</td>\n",
       "      <td>0.104856</td>\n",
       "      <td>0.085388</td>\n",
       "      <td>0.847202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.393128</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>0.160186</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.860811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.230992</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.871996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1425, training_loss=0.24291555839672424, metrics={'train_runtime': 1466.7503, 'train_samples_per_second': 7.772, 'train_steps_per_second': 0.972, 'total_flos': 76468786882368.0, 'train_loss': 0.24291555839672424, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner-v3',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- этот эксперимент менее удачный: возможно повлияло качество разметки синтетических данных или пересечение только с двумя ner классами с изначальным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- подход с дообучением на задачу mlm и последующим дообучением на задачу ner дал более качественный результат. скорее всего при mlm модель начинает лучше улавливать взаимосвязи между словами в нашем корпусе, что позволяет ей показывать хорошие результаты при извлечении именованых сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
