{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/Collection5/'\n",
    "records = load_ne5(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'id', 1: 'text', 2: 'ner'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ñ–∏—Ä–∏–Ω–æ–≤—Å–∫–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–±–º–µ–Ω—è—Ç—å —Å –°–®–ê –°–Ω–æ—É–¥–µ–Ω–∞...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–î.–ú–µ–¥–≤–µ–¥–µ–≤ –Ω–∞–∑–Ω–∞—á–∏–ª —Ä—è–¥ –≥–ª–∞–≤ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ú–í–î\\...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–°–ú–ò: –í.–°—É—Ä–∫–æ–≤—É –Ω–∞–¥–æ–µ–ª–æ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏...</td>\n",
       "      <td>[Ne5Span(index='T1', type='MEDIA', start=0, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–î.–ú–µ–¥–≤–µ–¥–µ–≤ –æ—Å–≤–æ–±–æ–¥–∏–ª –æ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –µ—â–µ 10 –≥–µ–Ω–µ—Ä...</td>\n",
       "      <td>[Ne5Span(index='T1', type='PER', start=0, stop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  –ñ–∏—Ä–∏–Ω–æ–≤—Å–∫–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–±–º–µ–Ω—è—Ç—å —Å –°–®–ê –°–Ω–æ—É–¥–µ–Ω–∞...   \n",
       "1  –î.–ú–µ–¥–≤–µ–¥–µ–≤ –Ω–∞–∑–Ω–∞—á–∏–ª —Ä—è–¥ –≥–ª–∞–≤ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ú–í–î\\...   \n",
       "2  –°–ú–ò: –í.–°—É—Ä–∫–æ–≤—É –Ω–∞–¥–æ–µ–ª–æ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏...   \n",
       "3  –î.–ú–µ–¥–≤–µ–¥–µ–≤ –æ—Å–≤–æ–±–æ–¥–∏–ª –æ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –µ—â–µ 10 –≥–µ–Ω–µ—Ä...   \n",
       "\n",
       "                                                 ner  \n",
       "0  [Ne5Span(index='T1', type='PER', start=0, stop...  \n",
       "1  [Ne5Span(index='T1', type='PER', start=0, stop...  \n",
       "2  [Ne5Span(index='T1', type='MEDIA', start=0, st...  \n",
       "3  [Ne5Span(index='T1', type='PER', start=0, stop...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ 5 —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π: ['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'PER']\n",
      "–í—Å–µ–≥–æ 11 –º–µ—Ç–æ–∫: {0: 'O', 1: 'B-GEOPOLIT', 2: 'I-GEOPOLIT', 3: 'B-LOC', 4: 'I-LOC', 5: 'B-MEDIA', 6: 'I-MEDIA', 7: 'B-ORG', 8: 'I-ORG', 9: 'B-PER', 10: 'I-PER'}\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–∏–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "entity_types = set()\n",
    "for spans in df['ner']:\n",
    "    for span in spans:\n",
    "        entity_types.add(span.type)\n",
    "\n",
    "# –°–æ–∑–¥–∞–¥–∏–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –º–µ—Ç–æ–∫\n",
    "id2label = {0: \"O\"}  # O - –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –≤—Ö–æ–¥—è—â–∏—Ö –≤ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å\n",
    "for i, entity_type in enumerate(sorted(entity_types), 1):\n",
    "    id2label[2*i-1] = f\"B-{entity_type}\"  # B- –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "    id2label[2*i] = f\"I-{entity_type}\"    # I- –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "    \n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"–í—Å–µ–≥–æ {len(entity_types)} —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π: {sorted(entity_types)}\")\n",
    "print(f\"–í—Å–µ–≥–æ {len(id2label)} –º–µ—Ç–æ–∫: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ NER-—Ä–∞–∑–º–µ—Ç–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "def preprocess_data(examples):\n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "    text = examples[\"text\"]\n",
    "    spans = examples[\"ner\"]\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'O')\n",
    "    char_labels = [\"O\"] * len(text)\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "    for span in spans:\n",
    "        entity_type = span.type\n",
    "        start, end = span.start, span.stop\n",
    "        \n",
    "        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º B- –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "        char_labels[start] = f\"B-{entity_type}\"\n",
    "        \n",
    "        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º I- –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "        for i in range(start + 1, end):\n",
    "            char_labels[i] = f\"I-{entity_type}\"\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "    tokenized = tokenizer(text, truncation=True, return_offsets_mapping=True)\n",
    "    labels = []\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∏–º–≤–æ–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ –º–µ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    for i, (start, end) in enumerate(tokenized.offset_mapping):\n",
    "        # –ü—Ä–æ–ø—É—Å–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ([CLS], [SEP], ...)\n",
    "        if start == end == 0:\n",
    "            labels.append(-100)  # -100 –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "            continue\n",
    "            \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –º–µ—Ç–∫—É –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤ –≤ —ç—Ç–æ–º —Ç–æ–∫–µ–Ω–µ\n",
    "        token_labels = char_labels[start:end]\n",
    "        if not token_labels:\n",
    "            labels.append(-100)\n",
    "            continue\n",
    "            \n",
    "        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ B- –º–µ—Ç–∫–∞, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë\n",
    "        b_labels = [l for l in token_labels if l.startswith(\"B-\")]\n",
    "        if b_labels:\n",
    "            labels.append(label2id[b_labels[0]])\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ I- –º–µ—Ç–∫–∞, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é I- –º–µ—Ç–∫—É\n",
    "            i_labels = [l for l in token_labels if l.startswith(\"I-\")]\n",
    "            if i_labels:\n",
    "                labels.append(label2id[i_labels[0]])\n",
    "            else:\n",
    "                # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º \"O\"\n",
    "                labels.append(label2id[\"O\"])\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º offset_mapping, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-GEOPOLIT': 1,\n",
       " 'I-GEOPOLIT': 2,\n",
       " 'B-LOC': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-MEDIA': 5,\n",
       " 'I-MEDIA': 6,\n",
       " 'B-ORG': 7,\n",
       " 'I-ORG': 8,\n",
       " 'B-PER': 9,\n",
       " 'I-PER': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞–º\n",
    "processed_train_data = []\n",
    "processed_test_data = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    processed_train_data.append(preprocess_data(row))\n",
    "    \n",
    "for i, row in test_df.iterrows():\n",
    "    processed_test_data.append(preprocess_data(row))\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_train_data],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_train_data],\n",
    "    \"labels\": [x[\"labels\"] for x in processed_train_data]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_test_data],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_test_data],\n",
    "    \"labels\": [x[\"labels\"] for x in processed_test_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç—Ä–∏–∫–∏ –¥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3831734657287598,\n",
       " 'eval_model_preparation_time': 0.0004,\n",
       " 'eval_precision': 0.006539280014186913,\n",
       " 'eval_recall': 0.057071000193461015,\n",
       " 'eval_f1': 0.01173405461307452,\n",
       " 'eval_accuracy': 0.12865108868826342,\n",
       " 'eval_runtime': 4.9607,\n",
       " 'eval_samples_per_second': 40.317,\n",
       " 'eval_steps_per_second': 5.04}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models',\n",
    "    per_device_eval_batch_size=8,\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"–ú–µ—Ç—Ä–∏–∫–∏ –¥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è:\")\n",
    "pre_training_metrics = trainer.evaluate(test_dataset)\n",
    "pre_training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_28611/2943834762.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588897</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.250339</td>\n",
       "      <td>0.325617</td>\n",
       "      <td>0.843717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423745</td>\n",
       "      <td>0.441952</td>\n",
       "      <td>0.369704</td>\n",
       "      <td>0.402612</td>\n",
       "      <td>0.868262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386830</td>\n",
       "      <td>0.472085</td>\n",
       "      <td>0.444960</td>\n",
       "      <td>0.458122</td>\n",
       "      <td>0.885422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6780633036295572, metrics={'train_runtime': 488.9889, 'train_samples_per_second': 4.908, 'train_steps_per_second': 0.614, 'total_flos': 22374206344752.0, 'train_loss': 0.6780633036295572, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3868299722671509,\n",
       " 'eval_precision': 0.4720853858784893,\n",
       " 'eval_recall': 0.444960340491391,\n",
       " 'eval_f1': 0.45812170102579425,\n",
       " 'eval_accuracy': 0.8854221986192247,\n",
       " 'eval_runtime': 4.3761,\n",
       " 'eval_samples_per_second': 45.703,\n",
       " 'eval_steps_per_second': 5.713,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "post_training_metrics = trainer.evaluate(test_dataset)\n",
    "post_training_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   MLM –¥–æ–æ–±—É—á–µ–Ω–∏–µ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞–º\n",
    "processed_train_data_mlm = []\n",
    "processed_test_data_mlm = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    processed_train_data_mlm.append(preprocess_function(row))\n",
    "    \n",
    "for i, row in test_df.iterrows():\n",
    "    processed_test_data_mlm.append(preprocess_function(row))\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã Hugging Face\n",
    "train_dataset_mlm = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_train_data_mlm],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_train_data_mlm],\n",
    "})\n",
    "\n",
    "test_dataset_mlm = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"] for x in processed_test_data_mlm],\n",
    "    \"attention_mask\": [x[\"attention_mask\"] for x in processed_test_data_mlm],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce5a1822861441ba03968cf9493d434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f529877c572b4606a8d6072fefa9f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_mlm_grouped = train_dataset_mlm.map(group_texts, batched=True, num_proc=4)\n",
    "test_dataset_mlm_grouped = test_dataset_mlm.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 05:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.191179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.124142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.100579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=372, training_loss=3.4454778855846775, metrics={'train_runtime': 317.2932, 'train_samples_per_second': 18.683, 'train_steps_per_second': 1.172, 'total_flos': 11310209703936.0, 'train_loss': 3.4454778855846775, 'epoch': 3.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/models/rubert-tiny2-mlm\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_mlm_grouped,\n",
    "    eval_dataset=test_dataset_mlm_grouped,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 23.54\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER –¥–æ–æ–±—É—á–µ–Ω–∏–µ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../data/models/rubert-tiny2-mlm/checkpoint-372 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    '../data/models/rubert-tiny2-mlm/checkpoint-372', \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner-v2',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_28611/2943834762.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 08:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.576352</td>\n",
       "      <td>0.459727</td>\n",
       "      <td>0.260592</td>\n",
       "      <td>0.332634</td>\n",
       "      <td>0.843949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.413024</td>\n",
       "      <td>0.442426</td>\n",
       "      <td>0.390985</td>\n",
       "      <td>0.415118</td>\n",
       "      <td>0.876958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375955</td>\n",
       "      <td>0.491532</td>\n",
       "      <td>0.471658</td>\n",
       "      <td>0.481390</td>\n",
       "      <td>0.895031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6769447835286458, metrics={'train_runtime': 487.8083, 'train_samples_per_second': 4.92, 'train_steps_per_second': 0.615, 'total_flos': 22374206344752.0, 'train_loss': 0.6769447835286458, 'epoch': 3.0})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ —Å –ø—Ä–æ—Å—Ç—ã–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ ner –∑–∞–¥–∞—á—É, –ø–æ–ª—É—á–∏–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—à–µ –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –æ–±—É—á–µ–Ω–∏–µ —Å –¥–æ–ø —Ä–∞–∑–º–µ—Ç–∫–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_10000_labeled = pd.read_csv('../data/lenta_10000_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_data(text, entities):\n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, padding=False, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∫–∞–∫ \"O\" (outside)\n",
    "    labels = [\"O\"] * len(input_ids)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ –∫ –∏–Ω–¥–µ–∫—Å—É —Ç–æ–∫–µ–Ω–∞\n",
    "    position_to_token_idx = {}\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if start != end:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω—ã —Å –Ω—É–ª–µ–≤–æ–π –¥–ª–∏–Ω–æ–π\n",
    "            for pos in range(start, end):\n",
    "                position_to_token_idx[pos] = i\n",
    "    \n",
    "    # –†–∞–∑–º–µ—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö entities\n",
    "    for entity in entities:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "        start_pos = entity['start']\n",
    "        end_pos = entity['end']\n",
    "        entity_label = entity['entity']\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã\n",
    "        token_indices = []\n",
    "        for pos in range(start_pos, end_pos):\n",
    "            if pos in position_to_token_idx:\n",
    "                token_idx = position_to_token_idx[pos]\n",
    "                if token_idx not in token_indices:\n",
    "                    token_indices.append(token_idx)\n",
    "        \n",
    "        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã, –ø—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–∫–∏\n",
    "        if token_indices:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –º–µ—Ç–∫–∏\n",
    "            if '-' in entity_label:\n",
    "                _, entity_type = entity_label.split('-', 1)\n",
    "            else:\n",
    "                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –º–µ—Ç–∫–∞ –Ω–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ X-TYPE\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª-–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            for i, token_idx in enumerate(token_indices):\n",
    "                if len(token_indices) == 1:\n",
    "                    labels[token_idx] = f\"U-{entity_type}\"\n",
    "                elif i == 0:\n",
    "                    labels[token_idx] = f\"B-{entity_type}\"\n",
    "                elif i == len(token_indices) - 1:\n",
    "                    labels[token_idx] = f\"L-{entity_type}\"\n",
    "                else:\n",
    "                    labels[token_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": [1] * len(input_ids),\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π\n",
    "def parse_entities(entities_str):\n",
    "    if isinstance(entities_str, str):\n",
    "        return ast.literal_eval(entities_str)\n",
    "    return entities_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 0 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 8900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9000 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9100 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9200 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9300 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9400 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9500 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9600 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9700 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9800 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 9900 —Å—Ç—Ä–æ–∫ –∏–∑ 10000\n"
     ]
    }
   ],
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "for idx, row in lenta_10000_labeled.iterrows():\n",
    "    try:\n",
    "        text_data = prepare_ner_data(row['text'], parse_entities(row['text_entities']))        \n",
    "        lenta_train_data.append(text_data)\n",
    "            \n",
    "        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx} —Å—Ç—Ä–æ–∫ –∏–∑ {len(lenta_10000_labeled)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ {idx}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "lenta_train_dataset = Dataset.from_dict({\n",
    "    'input_ids': [x['input_ids'] for x in lenta_train_data],\n",
    "    'attention_mask': [x['attention_mask'] for x in lenta_train_data],\n",
    "    'labels': [x['labels'] for x in lenta_train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lenta_labels_to_train_format(example):\n",
    "    # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ lenta –≤ —Ç–∏–ø—ã train\n",
    "    entity_type_mapping = {\n",
    "        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã -> LOC\n",
    "        'CITY': 'LOC',\n",
    "        'COUNTRY': 'LOC',\n",
    "        'DISTRICT': 'LOC',\n",
    "        'REGION': 'LOC',\n",
    "        'STREET': 'LOC',\n",
    "        'HOUSE': 'LOC',  # –î–æ–º–∞ —Ç–∞–∫–∂–µ –æ—Ç–Ω–æ—Å–∏–º –∫ –ª–æ–∫–∞—Ü–∏—è–º\n",
    "        \n",
    "        # –ü–µ—Ä—Å–æ–Ω—ã -> PER\n",
    "        'FIRST_NAME': 'PER',\n",
    "        'LAST_NAME': 'PER',\n",
    "        'MIDDLE_NAME': 'PER',\n",
    "        \n",
    "        # –î—Ä—É–≥–∏–µ —Ç–∏–ø—ã –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    }\n",
    "    \n",
    "    # –°–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫ train_dataset\n",
    "    train_label2id = {\n",
    "        'O': 0, \n",
    "        'B-GEOPOLIT': 1, 'I-GEOPOLIT': 2, \n",
    "        'B-LOC': 3, 'I-LOC': 4, \n",
    "        'B-MEDIA': 5, 'I-MEDIA': 6, \n",
    "        'B-ORG': 7, 'I-ORG': 8, \n",
    "        'B-PER': 9, 'I-PER': 10\n",
    "    }\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏\n",
    "    new_labels = []\n",
    "    for label in example['labels']:\n",
    "        if label == 'O':\n",
    "            new_labels.append(train_label2id['O'])\n",
    "            continue\n",
    "            \n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º –º–µ—Ç–∫—É –Ω–∞ –ø–æ–∑–∏—Ü–∏—é (B/I/L/U) –∏ —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "        position, entity_type = label.split('-', 1)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "        mapped_type = entity_type_mapping.get(entity_type)\n",
    "        if not mapped_type:\n",
    "            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, —Å—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω –Ω–µ-—Å—É—â–Ω–æ—Å—Ç—å—é\n",
    "            new_labels.append(train_label2id['O'])\n",
    "            continue\n",
    "            \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BIOLU -> BI\n",
    "        if position in ['B', 'U']:  # –ù–∞—á–∞–ª–æ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–ª–∏ –µ–¥–∏–Ω–∏—á–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å -> B\n",
    "            new_label = f'B-{mapped_type}'\n",
    "        elif position in ['I', 'L']:  # –í–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—É—â–Ω–æ—Å—Ç–∏ -> I\n",
    "            new_label = f'I-{mapped_type}'\n",
    "        else:\n",
    "            new_label = 'O'\n",
    "            \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç\n",
    "        new_labels.append(train_label2id.get(new_label, train_label2id['O']))\n",
    "        \n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä\n",
    "    example['labels'] = new_labels\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ LOC –∏ PER, —Ç–∫ –¥—Ä—É–≥–∏—Ö –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –Ω–∞—à —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414176dfaebd47b59d13e5b6d3222545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenta_train_dataset = lenta_train_dataset.map(convert_lenta_labels_to_train_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3000 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "lenta_train_dataset = lenta_train_dataset.select(range(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "combined_train_dataset = concatenate_datasets([lenta_train_dataset, train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/r8_rgfms4txcy1_nl4l107kc0000gn/T/ipykernel_38013/3256264858.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1425' max='1425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1425/1425 24:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457694</td>\n",
       "      <td>0.072017</td>\n",
       "      <td>0.104856</td>\n",
       "      <td>0.085388</td>\n",
       "      <td>0.847202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.393128</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>0.160186</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.860811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.230992</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.871996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1425, training_loss=0.24291555839672424, metrics={'train_runtime': 1466.7503, 'train_samples_per_second': 7.772, 'train_steps_per_second': 0.972, 'total_flos': 76468786882368.0, 'train_loss': 0.24291555839672424, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/models/rubert-tiny2-ner-v3',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- —ç—Ç–æ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –º–µ–Ω–µ–µ —É–¥–∞—á–Ω—ã–π: –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–≤–ª–∏—è–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å –¥–≤—É–º—è ner –∫–ª–∞—Å—Å–∞–º–∏ —Å –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –ø–æ–¥—Ö–æ–¥ —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –∑–∞–¥–∞—á—É mlm –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –∑–∞–¥–∞—á—É ner –¥–∞–ª –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—Ä–∏ mlm –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –≤ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–º–µ–Ω–æ–≤–∞–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
