{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Названы регионы России с самой высокой смертно...</td>\n",
       "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Австрия не представила доказательств вины росс...</td>\n",
       "      <td>Австрийские правоохранительные органы не предс...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Обнаружено самое счастливое место на планете</td>\n",
       "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
       "      <td>Путешествия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
       "      <td>С начала расследования российского вмешательст...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
       "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Названы регионы России с самой высокой смертно...   \n",
       "1  Австрия не представила доказательств вины росс...   \n",
       "2       Обнаружено самое счастливое место на планете   \n",
       "3  В США раскрыли сумму расходов на расследование...   \n",
       "4  Хакеры рассказали о планах Великобритании зами...   \n",
       "\n",
       "                                                text        topic  \n",
       "0  Вице-премьер по социальным вопросам Татьяна Го...       Россия  \n",
       "1  Австрийские правоохранительные органы не предс...        Спорт  \n",
       "2  Сотрудники социальной сети Instagram проанализ...  Путешествия  \n",
       "3  С начала расследования российского вмешательст...          Мир  \n",
       "4  Хакерская группировка Anonymous опубликовала н...          Мир  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import corus\n",
    "import wget\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "url = \"https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\"\n",
    "filename = \"lenta-ru-news.csv.gz\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    wget.download(url)\n",
    "\n",
    "records = corus.load_lenta(filename)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df.columns = [\"url\", \"title\", \"text\", \"topic\", \"tags\", \"extra\"]\n",
    "\n",
    "df = df[[\"title\", \"text\", \"topic\"]]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739351, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               21.710798\n",
       "Мир                  18.486483\n",
       "Экономика            10.757813\n",
       "Спорт                 8.713182\n",
       "Культура              7.277058\n",
       "Бывший СССР           7.222821\n",
       "Наука и техника       7.186844\n",
       "Интернет и СМИ        6.042462\n",
       "Из жизни              3.734491\n",
       "Дом                   2.939605\n",
       "Силовые структуры     2.650433\n",
       "Ценности              1.050381\n",
       "Бизнес                1.000743\n",
       "Путешествия           0.866706\n",
       "69-я параллель        0.171502\n",
       "Крым                  0.090079\n",
       "Культпросвет          0.045986\n",
       "                      0.027457\n",
       "Легпром               0.015419\n",
       "Библиотека            0.008791\n",
       "Оружие                0.000406\n",
       "ЧМ-2014               0.000271\n",
       "МедНовости            0.000135\n",
       "Сочи                  0.000135\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"topic\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Создание стратифицированной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер выборки: 100000 строк\n",
      "\n",
      "Распределение топиков в выборке (%):\n",
      "topic\n",
      "Россия               21.711\n",
      "Мир                  18.487\n",
      "Экономика            10.758\n",
      "Спорт                 8.713\n",
      "Культура              7.277\n",
      "Бывший СССР           7.223\n",
      "Наука и техника       7.187\n",
      "Интернет и СМИ        6.042\n",
      "Из жизни              3.735\n",
      "Дом                   2.940\n",
      "Силовые структуры     2.650\n",
      "Ценности              1.050\n",
      "Бизнес                1.001\n",
      "Путешествия           0.867\n",
      "69-я параллель        0.172\n",
      "Крым                  0.090\n",
      "Культпросвет          0.046\n",
      "                      0.027\n",
      "Легпром               0.015\n",
      "Библиотека            0.009\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Исходное распределение топиков (%):\n",
      "topic\n",
      "Россия               21.710856\n",
      "Мир                  18.486533\n",
      "Экономика            10.757842\n",
      "Спорт                 8.713206\n",
      "Культура              7.277078\n",
      "Бывший СССР           7.222841\n",
      "Наука и техника       7.186863\n",
      "Интернет и СМИ        6.042478\n",
      "Из жизни              3.734502\n",
      "Дом                   2.939613\n",
      "Силовые структуры     2.650440\n",
      "Ценности              1.050384\n",
      "Бизнес                1.000745\n",
      "Путешествия           0.866708\n",
      "69-я параллель        0.171502\n",
      "Крым                  0.090079\n",
      "Культпросвет          0.045986\n",
      "                      0.027457\n",
      "Легпром               0.015419\n",
      "Библиотека            0.008792\n",
      "Оружие                0.000406\n",
      "ЧМ-2014               0.000271\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Разница между распределениями (процентные пункты):\n",
      "topic\n",
      "Из жизни             0.000498\n",
      "69-я параллель       0.000498\n",
      "Интернет и СМИ       0.000478\n",
      "Мир                  0.000467\n",
      "                     0.000457\n",
      "Силовые структуры    0.000440\n",
      "Легпром              0.000419\n",
      "Дом                  0.000387\n",
      "Ценности             0.000384\n",
      "Путешествия          0.000292\n",
      "Бизнес               0.000255\n",
      "Библиотека           0.000208\n",
      "Спорт                0.000206\n",
      "Бывший СССР          0.000159\n",
      "Экономика            0.000158\n",
      "Россия               0.000144\n",
      "Наука и техника      0.000137\n",
      "Крым                 0.000079\n",
      "Культура             0.000078\n",
      "Культпросвет         0.000014\n",
      "Оружие                    NaN\n",
      "ЧМ-2014                   NaN\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Создаем стратифицированную выборку из 100 000 строк\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_size = 100000\n",
    "random_state = 42\n",
    "\n",
    "# Находим топики с количеством элементов >= 2\n",
    "topic_counts = df[\"topic\"].value_counts()\n",
    "valid_topics = topic_counts[topic_counts >= 2].index.tolist()\n",
    "\n",
    "df_filtered = df[df[\"topic\"].isin(valid_topics)]\n",
    "\n",
    "# Получаем стратифицированную выборку\n",
    "df_sample, _ = train_test_split(\n",
    "    df_filtered,\n",
    "    train_size=sample_size,\n",
    "    stratify=df_filtered[\"topic\"],\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "print(f\"Размер выборки: {df_sample.shape[0]} строк\")\n",
    "\n",
    "topic_distribution = df_sample[\"topic\"].value_counts(normalize=True) * 100\n",
    "print(\"\\nРаспределение топиков в выборке (%):\")\n",
    "print(topic_distribution)\n",
    "\n",
    "original_distribution = df_filtered[\"topic\"].value_counts(normalize=True) * 100\n",
    "print(\"\\nИсходное распределение топиков (%):\")\n",
    "print(original_distribution)\n",
    "\n",
    "print(\"\\nРазница между распределениями (процентные пункты):\")\n",
    "diff = topic_distribution - original_distribution\n",
    "print(diff.abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Нормализация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    # Замена множественных пробелов одиночными\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Удаление пробелов в начале и конце\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "df_sample[\"title_norm\"] = df_sample[\"title\"].apply(normalize_text)\n",
    "df_sample[\"text_norm\"] = df_sample[\"text\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "except:\n",
    "    nltk.download(\"stopwords\")\n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Удаление стоп-слов\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in russian_stopwords]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "df_sample[\"title_clean\"] = df_sample[\"title_norm\"].apply(clean_text)\n",
    "df_sample[\"text_clean\"] = df_sample[\"text_norm\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Государственный департамент США заявил, что заявления Арафата, призывающего палестинцев прекратить теракты против граждан Израиля, не достаточно для того, чтобы прекратить насилие, сообщает Ha'aretz. \"Заявление Арафата, осуждающее взрывы террористов-самоубийц является шагом в правильном направлении, - говорится в заявлении департамента. - Но очевидно, что следует сделать гораздо больше для того, чтобы прекратить насилие и террор\". Представитель Белого Дома Ари Флейшер (Ari Fleisher) также заявил, что президент Буш ждет от Арафата не риторики, а реальных действий. Руководство Израиля, со своей стороны, заявило, что осуждение терактов является со стороны Арафата лишь лицемерием, так как на деле он и пальцем не двинет для того, чтобы действительно остановить террор.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.iloc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "государственный департамент сша заявил, что заявления арафата, призывающего палестинцев прекратить теракты против граждан израиля, не достаточно для того, чтобы прекратить насилие, сообщает ha'aretz. \"заявление арафата, осуждающее взрывы террористов-самоубийц является шагом в правильном направлении, - говорится в заявлении департамента. - но очевидно, что следует сделать гораздо больше для того, чтобы прекратить насилие и террор\". представитель белого дома ари флейшер (ari fleisher) также заявил, что президент буш ждет от арафата не риторики, а реальных действий. руководство израиля, со своей стороны, заявило, что осуждение терактов является со стороны арафата лишь лицемерием, так как на деле он и пальцем не двинет для того, чтобы действительно остановить террор.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.iloc[0].text_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "государственный департамент сша заявил, заявления арафата, призывающего палестинцев прекратить теракты против граждан израиля, достаточно того, прекратить насилие, сообщает ha'aretz. \"заявление арафата, осуждающее взрывы террористов-самоубийц является шагом правильном направлении, - говорится заявлении департамента. - очевидно, следует сделать гораздо того, прекратить насилие террор\". представитель белого дома ари флейшер (ari fleisher) также заявил, президент буш ждет арафата риторики, реальных действий. руководство израиля, своей стороны, заявило, осуждение терактов является стороны арафата лишь лицемерием, деле пальцем двинет того, действительно остановить террор.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.iloc[0].text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- в russian_stopwords есть некоторые слова, устранение которых может исказить или поменять смысл текста, например \"не\", \"ни\", \"нет\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords.remove(\"нет\")\n",
    "russian_stopwords.remove(\"не\")\n",
    "russian_stopwords.remove(\"ни\")\n",
    "russian_stopwords.remove(\"без\")\n",
    "russian_stopwords.remove(\"только\")\n",
    "russian_stopwords.remove(\"даже\")\n",
    "russian_stopwords.remove(\"совсем\")\n",
    "russian_stopwords.remove(\"почти\")\n",
    "russian_stopwords.remove(\"всегда\")\n",
    "russian_stopwords.remove(\"никогда\")\n",
    "russian_stopwords.remove(\"ничего\")\n",
    "russian_stopwords.remove(\"нельзя\")\n",
    "russian_stopwords.remove(\"более\")\n",
    "russian_stopwords.remove(\"больше\")\n",
    "russian_stopwords.remove(\"лучше\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[\"title_clean\"] = df_sample[\"title_norm\"].apply(clean_text)\n",
    "df_sample[\"text_clean\"] = df_sample[\"text_norm\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка лемматизация обычно предпочтительнее стемминга из-за богатой морфологии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    return \"\".join(lemmas).strip()\n",
    "\n",
    "\n",
    "df_sample[\"title_lemma\"] = df_sample[\"title_clean\"].apply(lemmatize_text)\n",
    "df_sample[\"text_lemma\"] = df_sample[\"text_clean\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "государственный департамент сша заявлять, заявление арафат, призывать палестинец прекращать теракт против гражданин израиль, не достаточно то, прекращать насилие, сообщать ha'aretz. \"заявление арафат, осуждать взрыв террорист-самоубийца являться шаг правильный направление, - говориться заявление департамент. - очевидно, следовать сделать гораздо много то, прекращать насилие террор\". представитель белый дом ари флейшер (ari fleisher) также заявлять, президент буш ждать арафат не риторика, реальный действие. руководство израиль, свой сторона, заявлять, осуждение теракт являться сторона арафат лишь лицемерие, дело палец не двинуть то, действительно останавливать террор.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.iloc[0].text_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Преобразование таргета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словарь меток: {0: '', 1: '69-я параллель', 2: 'Библиотека', 3: 'Бизнес', 4: 'Бывший СССР', 5: 'Дом', 6: 'Из жизни', 7: 'Интернет и СМИ', 8: 'Крым', 9: 'Культпросвет ', 10: 'Культура', 11: 'Легпром', 12: 'Мир', 13: 'Наука и техника', 14: 'Путешествия', 15: 'Россия', 16: 'Силовые структуры', 17: 'Спорт', 18: 'Ценности', 19: 'Экономика'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_sample[\"topic_encoded\"] = label_encoder.fit_transform(df_sample[\"topic\"])\n",
    "\n",
    "label_dict = dict(\n",
    "    zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_)\n",
    ")\n",
    "print(\"Словарь меток:\", label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 60000 строк (60.0%)\n",
      "Размер валидационной выборки: 20000 строк (20.0%)\n",
      "Размер тестовой выборки: 20000 строк (20.0%)\n",
      "\n",
      "Распределение классов в обучающей выборке:\n",
      "topic_encoded\n",
      "0      0.026667\n",
      "1      0.171667\n",
      "2      0.008333\n",
      "3      1.001667\n",
      "4      7.223333\n",
      "5      2.940000\n",
      "6      3.735000\n",
      "7      6.041667\n",
      "8      0.090000\n",
      "9      0.046667\n",
      "10     7.276667\n",
      "11     0.015000\n",
      "12    18.486667\n",
      "13     7.186667\n",
      "14     0.866667\n",
      "15    21.711667\n",
      "16     2.650000\n",
      "17     8.713333\n",
      "18     1.050000\n",
      "19    10.758333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Распределение классов в валидационной выборке:\n",
      "topic_encoded\n",
      "0      0.030\n",
      "1      0.175\n",
      "2      0.010\n",
      "3      1.000\n",
      "4      7.220\n",
      "5      2.940\n",
      "6      3.735\n",
      "7      6.040\n",
      "8      0.090\n",
      "9      0.045\n",
      "10     7.280\n",
      "11     0.015\n",
      "12    18.490\n",
      "13     7.185\n",
      "14     0.870\n",
      "15    21.710\n",
      "16     2.650\n",
      "17     8.710\n",
      "18     1.050\n",
      "19    10.755\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Распределение классов в тестовой выборке:\n",
      "topic_encoded\n",
      "0      0.025\n",
      "1      0.170\n",
      "2      0.010\n",
      "3      1.000\n",
      "4      7.225\n",
      "5      2.940\n",
      "6      3.735\n",
      "7      6.045\n",
      "8      0.090\n",
      "9      0.045\n",
      "10     7.275\n",
      "11     0.015\n",
      "12    18.485\n",
      "13     7.190\n",
      "14     0.865\n",
      "15    21.710\n",
      "16     2.650\n",
      "17     8.715\n",
      "18     1.050\n",
      "19    10.760\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df_sample[[\"title_lemma\", \"text_lemma\"]]\n",
    "y = df_sample[\"topic_encoded\"]\n",
    "\n",
    "# train (60%) и temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# validation (50%) и test (50%) от temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=random_state, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Размер обучающей выборки: {X_train.shape[0]} строк ({X_train.shape[0]/len(X)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Размер валидационной выборки: {X_val.shape[0]} строк ({X_val.shape[0]/len(X)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Размер тестовой выборки: {X_test.shape[0]} строк ({X_test.shape[0]/len(X)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nРаспределение классов в обучающей выборке:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index() * 100)\n",
    "\n",
    "print(\"\\nРаспределение классов в валидационной выборке:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index() * 100)\n",
    "\n",
    "print(\"\\nРаспределение классов в тестовой выборке:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пайплайн обработки включает нормализацию (приведение к нижнему регистру), очистку от стоп-слов с сохранением ключевых смысловых частиц (отрицаний), лемматизацию как оптимальный метод для русского языка и стратифицированное разделение данных для сохранения пропорций классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/X_train.csv', index=False)\n",
    "X_val.to_csv('../data/X_val.csv', index=False)\n",
    "X_test.to_csv('../data/X_test.csv', index=False)\n",
    "\n",
    "y_train.to_csv('../data/y_train.csv', index=False)\n",
    "y_val.to_csv('../data/y_val.csv', index=False)\n",
    "y_test.to_csv('../data/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00       200\n",
      "           4       0.00      0.00      0.00      1444\n",
      "           5       0.00      0.00      0.00       588\n",
      "           6       0.00      0.00      0.00       747\n",
      "           7       0.00      0.00      0.00      1208\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.00      0.00      0.00      1456\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00      3698\n",
      "          13       0.00      0.00      0.00      1437\n",
      "          14       0.00      0.00      0.00       174\n",
      "          15       0.22      1.00      0.36      4342\n",
      "          16       0.00      0.00      0.00       530\n",
      "          17       0.00      0.00      0.00      1742\n",
      "          18       0.00      0.00      0.00       210\n",
      "          19       0.00      0.00      0.00      2151\n",
      "\n",
      "    accuracy                           0.22     20000\n",
      "   macro avg       0.01      0.05      0.02     20000\n",
      "weighted avg       0.05      0.22      0.08     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=random_state)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = dummy_clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TF-IDF/Bag of words + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.75      0.34      0.47        35\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.58      0.39      0.47       200\n",
      "           4       0.81      0.81      0.81      1444\n",
      "           5       0.84      0.83      0.83       588\n",
      "           6       0.62      0.56      0.59       747\n",
      "           7       0.77      0.71      0.74      1208\n",
      "           8       0.80      0.22      0.35        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.87      0.87      0.87      1456\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.78      0.81      0.80      3698\n",
      "          13       0.82      0.81      0.81      1437\n",
      "          14       0.80      0.70      0.74       174\n",
      "          15       0.76      0.82      0.79      4342\n",
      "          16       0.63      0.51      0.56       530\n",
      "          17       0.97      0.96      0.96      1742\n",
      "          18       0.91      0.80      0.85       210\n",
      "          19       0.82      0.84      0.83      2151\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.63      0.55      0.57     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(random_state=random_state)\n",
    "X_train_vec = vectorizer.fit_transform(\n",
    "    X_train[\"title_lemma\"] + \" \" + X_train[\"text_lemma\"]\n",
    ")\n",
    "X_val_vec = vectorizer.transform(X_val[\"title_lemma\"] + \" \" + X_val[\"text_lemma\"])\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "lr_clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_val_pred = lr_clf.predict(X_val_vec)\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.70      0.14      0.23       200\n",
      "           4       0.82      0.81      0.81      1444\n",
      "           5       0.86      0.77      0.81       588\n",
      "           6       0.66      0.52      0.58       747\n",
      "           7       0.80      0.72      0.76      1208\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.86      0.89      0.88      1456\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.79      0.85      0.82      3698\n",
      "          13       0.82      0.85      0.83      1437\n",
      "          14       0.82      0.49      0.62       174\n",
      "          15       0.76      0.85      0.80      4342\n",
      "          16       0.73      0.33      0.46       530\n",
      "          17       0.97      0.96      0.96      1742\n",
      "          18       0.94      0.70      0.80       210\n",
      "          19       0.81      0.86      0.83      2151\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.57      0.49      0.51     20000\n",
      "weighted avg       0.80      0.81      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(random_state=random_state)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(\n",
    "    X_train[\"title_lemma\"] + \" \" + X_train[\"text_lemma\"]\n",
    ")\n",
    "X_val_tfidf = tfidf_vectorizer.transform(\n",
    "    X_val[\"title_lemma\"] + \" \" + X_val[\"text_lemma\"]\n",
    ")\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "lr_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_val_pred = lr_clf.predict(X_val_tfidf)\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем поиск оптимальных гиперпараметров...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(24160) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24161) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24162) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24163) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24164) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24165) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.2min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.2min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.2min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.3min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x14d8777e0>, vect__vectorizer_type=count; total time=  52.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x12e64b7e0>, vect__vectorizer_type=count; total time=  50.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x14d8cb240>, vect__vectorizer_type=count; total time=  50.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x12f1676a0>, vect__vectorizer_type=count; total time=  47.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x13779f240>, vect__vectorizer_type=count; total time=  57.2s\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x12e69f6a0>, vect__vectorizer_type=tfidf; total time=  58.6s\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x11dc5ec00>, vect__vectorizer_type=tfidf; total time=  56.5s\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x10fc6b380>, vect__vectorizer_type=tfidf; total time= 1.1min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x14d2ce980>, vect__vectorizer_type=tfidf; total time= 1.1min\n",
      "[CV] END vect__max_df=0.5, vect__tokenizer=<function custom_tokenize at 0x12ef2e980>, vect__vectorizer_type=tfidf; total time= 1.1min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.3min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.2min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.2min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.3min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=count; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=None, vect__vectorizer_type=tfidf; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x10450f6a0>, vect__vectorizer_type=count; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x12ee77880>, vect__vectorizer_type=count; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x1373e72e0>, vect__vectorizer_type=count; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x11d8872e0>, vect__vectorizer_type=count; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kulyaskin_mikhail/Library/Caches/pypoetry/virtualenvs/dl-nlp-7PEEyLUf-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x12e2e32e0>, vect__vectorizer_type=count; total time= 1.1min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x10f9bf2e0>, vect__vectorizer_type=tfidf; total time= 1.2min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x100f73ec0>, vect__vectorizer_type=tfidf; total time= 1.0min\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x12f13e7a0>, vect__vectorizer_type=tfidf; total time=  57.8s\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x100cfbec0>, vect__vectorizer_type=tfidf; total time=  48.1s\n",
      "[CV] END vect__max_df=0.7, vect__tokenizer=<function custom_tokenize at 0x11d89cc20>, vect__vectorizer_type=tfidf; total time=  47.4s\n",
      "\n",
      "Лучшие параметры:\n",
      "{'vect__vectorizer_type': 'tfidf', 'vect__tokenizer': None, 'vect__max_df': 0.5}\n",
      "\n",
      "Лучший f1-weighted score: 0.7962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def combine_features(X):\n",
    "    return X[\"title_lemma\"] + \" \" + X[\"text_lemma\"]\n",
    "\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    text = re.sub(r\"[^а-яА-Я ]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# Создаем класс, который выбирает между разными векторизаторами\n",
    "class VectorizerSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vectorizer_type=\"tfidf\", max_df=1.0, tokenizer=None, **kwargs):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.max_df = max_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.vectorizer_type == \"count\":\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                max_df=self.max_df, tokenizer=self.tokenizer, **self.kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_df=self.max_df, tokenizer=self.tokenizer, **self.kwargs\n",
    "            )\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", VectorizerSelector()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000, n_jobs=6)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"vect__vectorizer_type\": [\"count\", \"tfidf\"],\n",
    "    \"vect__max_df\": [0.5, 0.7],\n",
    "    \"vect__tokenizer\": [None, custom_tokenize],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=2,\n",
    "    n_jobs=6,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "X_train_combined = combine_features(X_train)\n",
    "\n",
    "print(\"Начинаем поиск оптимальных гиперпараметров...\")\n",
    "random_search.fit(X_train_combined, y_train)\n",
    "\n",
    "print(\"\\nЛучшие параметры:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\nЛучший f1-weighted score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры:\n",
    "{'vect__vectorizer_type': 'tfidf', 'vect__tokenizer': None, 'vect__max_df': 0.5}\n",
    "\n",
    "Лучший f1-weighted score: 0.7962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Оценка на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты на тестовой выборке:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       1.00      0.06      0.11        34\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.88      0.22      0.35       200\n",
      "           4       0.81      0.82      0.82      1445\n",
      "           5       0.86      0.74      0.79       588\n",
      "           6       0.71      0.56      0.63       747\n",
      "           7       0.78      0.72      0.75      1209\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.85      0.88      0.87      1455\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.80      0.85      0.82      3697\n",
      "          13       0.83      0.85      0.84      1438\n",
      "          14       0.78      0.43      0.56       173\n",
      "          15       0.77      0.85      0.80      4342\n",
      "          16       0.71      0.34      0.46       530\n",
      "          17       0.96      0.96      0.96      1743\n",
      "          18       0.92      0.70      0.79       210\n",
      "          19       0.82      0.87      0.85      2152\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.62      0.49      0.52     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_combined = combine_features(X_test)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_combined)\n",
    "\n",
    "print(\"\\nРезультаты на тестовой выборке:\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-nlp-7PEEyLUf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
